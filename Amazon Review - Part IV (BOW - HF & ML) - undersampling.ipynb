{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3080c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.2.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/didemekinci/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/ipykernel_launcher.py:22: FutureWarning:\n",
      "\n",
      "Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "\n",
      "/Users/didemekinci/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning:\n",
      "\n",
      "The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "\n",
      "/Users/didemekinci/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/ipykernel_launcher.py:47: FutureWarning:\n",
      "\n",
      "Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "\n",
    "#plotly library\n",
    "import plotly as py\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from time import time\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "from pprint import pprint\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=1.3)\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.externals \n",
    "import joblib\n",
    "import gensim\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from time import time\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "from pprint import pprint\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=1.3)\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.externals \n",
    "import joblib\n",
    "import gensim\n",
    "\n",
    "#matplotlib library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#word cloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#from a unix time to a date\n",
    "from time import strftime\n",
    "from datetime import datetime\n",
    "import pandas as pd \n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "import string\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns\n",
    "# Importing the Required Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#from xgboost import XGBClassifier\n",
    "#from lightgbm import LGBMModel,LGBMClassifier, plot_importance\n",
    "#from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2377e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1094466 entries, 1 to 1094466\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count    Dtype  \n",
      "---  ------             --------------    -----  \n",
      " 0   marketplace        1094466 non-null  object \n",
      " 1   customer_id        1094466 non-null  int64  \n",
      " 2   review_id          1094466 non-null  object \n",
      " 3   product_id         1094466 non-null  object \n",
      " 4   product_parent     1094466 non-null  int64  \n",
      " 5   product_title      1094466 non-null  object \n",
      " 6   product_category   1094466 non-null  object \n",
      " 7   star_rating        1094464 non-null  object \n",
      " 8   helpful_votes      1094461 non-null  float64\n",
      " 9   total_votes        1094461 non-null  float64\n",
      " 10  vine               1094461 non-null  object \n",
      " 11  verified_purchase  1094461 non-null  object \n",
      " 12  review_headline    1094454 non-null  object \n",
      " 13  review_body        1094449 non-null  object \n",
      " 14  review_date        1094407 non-null  object \n",
      "dtypes: float64(2), int64(2), object(11)\n",
      "memory usage: 133.6+ MB\n"
     ]
    }
   ],
   "source": [
    "mypath=\"/Users/didemekinci/Desktop/Graduate Project/02_data/\"\n",
    "import pandas as pd\n",
    "import glob\n",
    "files = glob.glob(\"/Users/didemekinci/Desktop/Graduate Project/02_data/*.csv\")\n",
    "column_names=['marketplace',\"customer_id\",\"review_id\",\"product_id\",\"product_parent\",\"product_title\",\"product_category\",\"star_rating\",\"helpful_votes\",\"total_votes\",\"vine\",\"verified_purchase\",\"review_headline\",\"review_body\",\"review_date\"]\n",
    "df0=pd.read_csv('/Users/didemekinci/Desktop/Graduate Project/02_data/0000000015.csv',sep=\"\\t\",names=column_names, low_memory=False)\n",
    "df1=pd.read_csv('/Users/didemekinci/Desktop/Graduate Project/02_data/0000000366.csv',sep=\"\\t\",names=column_names, low_memory=False)\n",
    "df2=pd.read_csv('/Users/didemekinci/Desktop/Graduate Project/02_data/0000000115.csv',sep=\"\\t\",names=column_names, low_memory=False)\n",
    "df3=pd.read_csv('/Users/didemekinci/Desktop/Graduate Project/02_data/0000000300.csv',sep=\"\\t\",names=column_names, low_memory=False)\n",
    "df4=pd.read_csv('/Users/didemekinci/Desktop/Graduate Project/02_data/0000000324.csv',sep=\"\\t\",names=column_names, low_memory=False)\n",
    "df5=pd.read_csv('/Users/didemekinci/Desktop/Graduate Project/02_data/0000000247.csv',sep=\"\\t\",names=column_names, low_memory=False)\n",
    "df6=pd.read_csv('/Users/didemekinci/Desktop/Graduate Project/02_data/0000000018.csv',sep=\"\\t\",names=column_names, low_memory=False)\n",
    "df7=pd.read_csv('/Users/didemekinci/Desktop/Graduate Project/02_data/0000000069.csv',sep=\"\\t\",names=column_names, low_memory=False)\n",
    "df8=pd.read_csv('/Users/didemekinci/Desktop/Graduate Project/02_data/0000000125.csv',sep=\"\\t\",names=column_names, low_memory=False)\n",
    "df9=pd.read_csv('/Users/didemekinci/Desktop/Graduate Project/02_data/0000000190.csv',sep=\"\\t\",names=column_names, low_memory=False)\n",
    "df10=pd.read_csv('/Users/didemekinci/Desktop/Graduate Project/02_data/0000000268.csv',sep=\"\\t\",names=column_names, low_memory=False)\n",
    "df = df0.append([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10])\n",
    "df.index = np.arange(1, len(df)+1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a26f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORRECTION OF COLUMNS AND TYPES\n",
    "df = df.drop(df[(df.review_date == \"1\") | (df.review_date == \"4\") | (df.review_date == \"2\") | (df.review_date == \"3\") | (df.review_date == \"5\")| (df.review_date.isnull())].index)\n",
    "df = df.drop(df[(df.star_rating == '2015-03-27')|(df.star_rating == \"2015-04-17\") | (df.star_rating == \"2011-07-10\") |  (df.star_rating.isnull())].index)\n",
    "df['helpful_votes'] = df['helpful_votes'].fillna(0)\n",
    "df['total_votes'] = df['total_votes'].fillna(0)\n",
    "df= df.astype({\"customer_id\": int, \"product_parent\": int, \"star_rating\": int, \"helpful_votes\" : int, \"total_votes\":float, \"review_body\": str})\n",
    "df['review_date']=pd.to_datetime(df['review_date'], format='%Y/%m/%d',errors =\"ignore\")\n",
    "\n",
    "#adding review length and binary vectorize star rating by comment columns\n",
    "df[\"review_len\"]=[len(x) for x in df['review_body']]\n",
    "df[\"comment\"] = df['star_rating'].apply(lambda x: 1 if x>3 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4bff18",
   "metadata": {},
   "source": [
    "# DATA CLEANSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "480c6a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    187068\n",
       "1    187068\n",
       "Name: comment, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True) #shuffle\n",
    "data=df[df['comment']==0][:187068]\n",
    "data=data.append(df[df['comment']==1][:187068])\n",
    "data = data.reset_index(drop=True)\n",
    "display(data['comment'].value_counts())\n",
    "df=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5cd7aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/didemekinci/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "sw = stopwords.words('english')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from string import punctuation\n",
    "\n",
    "def cleanText(text):\n",
    "    #text = BeautifulSoup(text, \"lxml\",).text\n",
    "    text = text.lower()\n",
    "\n",
    "    text= text.replace('<br />',' ')\n",
    "    text= text.replace('<br/>',' ')\n",
    "\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    # general\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r'\\|\\|\\| ', r'  ', text)\n",
    "    text = re.sub(r'http\\S+', r' <URL> ', text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}^`+=~|.!?,]\", \" \", text) \n",
    "    text = re.sub(r'[^a-zA-z.,!?/:;\\\"\\'\\s]', \" \", text)\n",
    "    text = re.sub(r'[^a-zA-z.,!?/:;\\\"\\'\\s]', \" \", text)\n",
    "    text = re.sub(r'[^\\w]', ' ', text)\n",
    "    text = re.sub(\"\\s\\s+\", \" \", text)\n",
    "   # text=re.sub('(\\\\b[A-Ka-k] \\\\b|\\\\b [A-Ka-k]\\\\b)', '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65628e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_review'] = df['review_body'].apply(cleanText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3e91f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(s):\n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    # remove short words, they're probably not useful\n",
    "    tokens = [t for t in tokens if len(t) > 2]\n",
    "    # put words into base form\n",
    "    #tokens = [t for t in tokens if t not in stopwords.words(\"english\") + [\"\"]]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8191815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_review'] = df['cleaned_review'].apply(tokenize_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "416c2e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokenized_string\"] = df.tokenized_review.apply(lambda x: ' '.join([str(i) for i in x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18c7f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_sentence(s):\n",
    "    \n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t, pos=\"v\") for t in s]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b15b3d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatized_review'] = df['tokenized_review'].apply(lemmatize_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f3f1c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lemmatized_string\"] = df.lemmatized_review.apply(lambda x: ' '.join([str(i) for i in x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b98450e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "df['lemmatized_string_wo_sw'] = df['lemmatized_string'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "df[\"tokenized_string_wo_sw\"] = df.tokenized_string.apply(lambda x: ''.join([str(i) for i in x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31892e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using lemmatized string for input and comment for output\n",
    "X=df[\"lemmatized_string\"]\n",
    "y=df[\"comment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0c115f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23ede23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COUNTVECTORIZER, HASHING VECTORIZER , BIGRAM VECTORIZER\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "hv = HashingVectorizer(alternate_sign=False)\n",
    "bigram = CountVectorizer(ngram_range=(2, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5807844",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vectorizer = count_vectorizer.fit_transform(X)\n",
    "X_hv = hv.fit_transform(X)\n",
    "X_bigram = bigram.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f35f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN TEST SPLIT FOR 80%-20% for each vectorizer\n",
    "train_X_vectorizer, test_X_vectorizer, train_y, test_y = train_test_split(X_vectorizer, y, test_size=0.2, random_state=0)\n",
    "train_X_hv, test_X_hv, train_y, test_y = train_test_split(X_hv, y, test_size=0.2, random_state=0)\n",
    "train_X_bigram, test_X_bigram, train_y, test_y = train_test_split(X_bigram, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f197006b",
   "metadata": {},
   "source": [
    "#COUNT VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "628af2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-reg & count vec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.81     37726\n",
      "           1       0.80      0.83      0.81     37102\n",
      "\n",
      "    accuracy                           0.81     74828\n",
      "   macro avg       0.81      0.81      0.81     74828\n",
      "weighted avg       0.81      0.81      0.81     74828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Lr_vect = LogisticRegression()\n",
    "Lr_vect.fit(train_X_vectorizer,train_y)\n",
    "Lr_pred_vect_train = Lr_vect.predict(train_X_vectorizer)\n",
    "Lr_pred_vect_test = Lr_vect.predict(test_X_vectorizer)\n",
    "print(\"log-reg & count vec\")\n",
    "print(classification_report(test_y, Lr_pred_vect_test, target_names=[\"0\",\"1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e05f8185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC & CountVEC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.78      0.80     37726\n",
      "           1       0.79      0.81      0.80     37102\n",
      "\n",
      "    accuracy                           0.80     74828\n",
      "   macro avg       0.80      0.80      0.80     74828\n",
      "weighted avg       0.80      0.80      0.80     74828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_vect = LinearSVC(random_state=0)\n",
    "SVM_vect.fit(train_X_vectorizer,train_y)\n",
    "SVM_vect_train = SVM_vect.predict(train_X_vectorizer)\n",
    "SVM_vect_test = SVM_vect.predict(test_X_vectorizer)\n",
    "print(\"SVC & CountVEC\")\n",
    "print(classification_report(test_y, SVM_vect_test, target_names=[\"0\",\"1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "035618fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB & CountVEC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80     37726\n",
      "           1       0.79      0.82      0.80     37102\n",
      "\n",
      "    accuracy                           0.80     74828\n",
      "   macro avg       0.80      0.80      0.80     74828\n",
      "weighted avg       0.80      0.80      0.80     74828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NB_vect = MultinomialNB()\n",
    "NB_vect.fit(train_X_vectorizer,train_y)\n",
    "NB_pred_vect_train = NB_vect.predict(train_X_vectorizer)\n",
    "NB_pred_vect_test = NB_vect.predict(test_X_vectorizer)\n",
    "print(\"MultinomialNB & CountVEC\")\n",
    "\n",
    "print(classification_report(test_y, NB_pred_vect_test, target_names=[\"0\",\"1\"]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "86815591",
   "metadata": {},
   "source": [
    "from sklearn.tree import  DecisionTreeClassifier\n",
    "dt_vect = DecisionTreeClassifier()\n",
    "\n",
    "dt_vect.fit(train_X_vectorizer,train_y)\n",
    "#dt_vect_train = dt_vect.predict(train_X_vectorizer)\n",
    "dt_vect_test = dt_vect.predict(test_X_vectorizer)\n",
    "\n",
    "print(\"DecisionTree & CountVEC\")\n",
    "\n",
    "print(classification_report(test_y, dt_vect_test, target_names=[\"0\",\"1\"]))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2a53e67",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_vec=RandomForestClassifier(n_estimators=30,random_state = 3)\n",
    "\n",
    "rf_vect.fit(train_X_vectorizer,train_y)\n",
    "rf_vex_train = rf_vec.predict(train_X_vectorizer)\n",
    "rf_vec_test = rf_vec.predict(test_X_vectorizer)\n",
    "\n",
    "print(\"RandomForest & CountVEC\")\n",
    "\n",
    "print(classification_report(test_y, rf_vec_test, target_names=[\"0\",\"1\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa976c3",
   "metadata": {},
   "source": [
    "#HASHING VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "721f504c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-reg & hashing vec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.85     37726\n",
      "           1       0.85      0.84      0.84     37102\n",
      "\n",
      "    accuracy                           0.85     74828\n",
      "   macro avg       0.85      0.85      0.85     74828\n",
      "weighted avg       0.85      0.85      0.85     74828\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-48c275ffa258>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlr_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X_hv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLr_pred_hv_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train ccuracy of RF:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X_hv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLr_pred_hv_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test accuracy of RF:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X_hv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'score'"
     ]
    }
   ],
   "source": [
    "#Hashing\n",
    "Lr_hv = LogisticRegression(max_iter=2000,solver=\"lbfgs\")\n",
    "lr_model=Lr_hv.fit(train_X_hv, train_y)\n",
    "Lr_pred_hv_train = Lr_hv.predict(train_X_hv)\n",
    "Lr_pred_hv_test = Lr_hv.predict(test_X_hv)\n",
    "print(\"log-reg & hashing vec\")\n",
    "print(classification_report(test_y, Lr_pred_hv_test, target_names=[\"0\",\"1\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73ca662d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC & hashing vec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85     37726\n",
      "           1       0.85      0.85      0.85     37102\n",
      "\n",
      "    accuracy                           0.85     74828\n",
      "   macro avg       0.85      0.85      0.85     74828\n",
      "weighted avg       0.85      0.85      0.85     74828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_vect = LinearSVC(random_state=0)\n",
    "SVM_vect.fit(train_X_hv, train_y)\n",
    "SVM_vect_train = SVM_vect.predict(train_X_hv)\n",
    "SVM_vect_test = SVM_vect.predict(test_X_hv)\n",
    "print(\"SVC & hashing vec\")\n",
    "print(classification_report(test_y, SVM_vect_test, target_names=[\"0\",\"1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "520f29e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB & CountVEC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.91      0.83     37726\n",
      "           1       0.88      0.73      0.80     37102\n",
      "\n",
      "    accuracy                           0.82     74828\n",
      "   macro avg       0.83      0.82      0.82     74828\n",
      "weighted avg       0.83      0.82      0.82     74828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NB_vect = MultinomialNB()\n",
    "NB_vect.fit(train_X_hv, train_y)\n",
    "NB_pred_vect_train = NB_vect.predict(train_X_hv)\n",
    "NB_pred_vect_test = NB_vect.predict(test_X_hv)\n",
    "print(\"MultinomialNB & CountVEC\")\n",
    "\n",
    "print(classification_report(test_y, NB_pred_vect_test, target_names=[\"0\",\"1\"]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f3824b6",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_vect=RandomForestClassifier()\n",
    "\n",
    "rf_vect.fit(train_X_hv, train_y)\n",
    "rf_vect_train = rf_vect.predict(train_X_hv)\n",
    "rf_vect_test = rf_vect.predict(train_X_hv)\n",
    "\n",
    "print(\"RandomForest & hashing vec\")\n",
    "\n",
    "print(classification_report(test_y, rf_vect_test, target_names=[\"0\",\"1\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c8bf31",
   "metadata": {},
   "source": [
    "#ngram vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22737744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-reg & ngram vec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85     37726\n",
      "           1       0.84      0.88      0.86     37102\n",
      "\n",
      "    accuracy                           0.85     74828\n",
      "   macro avg       0.86      0.85      0.85     74828\n",
      "weighted avg       0.86      0.85      0.85     74828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ngram\n",
    "Lr_bigram = LogisticRegression()\n",
    "Lr_bigram.fit(train_X_bigram, train_y)\n",
    "Lr_pred_bigram_train = Lr_bigram.predict(train_X_bigram)\n",
    "Lr_pred_bigram_test = Lr_bigram.predict(test_X_bigram)\n",
    "print(\"log-reg & ngram vec\")\n",
    "print(classification_report(test_y, Lr_pred_bigram_test, target_names=[\"0\",\"1\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd42138e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC & ngram vec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.81      0.83     37726\n",
      "           1       0.82      0.85      0.83     37102\n",
      "\n",
      "    accuracy                           0.83     74828\n",
      "   macro avg       0.83      0.83      0.83     74828\n",
      "weighted avg       0.83      0.83      0.83     74828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_vect = LinearSVC(random_state=0)\n",
    "SVM_vect.fit(train_X_bigram, train_y)\n",
    "SVM_vect_train = SVM_vect.predict(train_X_bigram)\n",
    "SVM_vect_test = SVM_vect.predict(test_X_bigram)\n",
    "print(\"SVC & ngram vec\")\n",
    "print(classification_report(test_y, SVM_vect_test, target_names=[\"0\",\"1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "292b525f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB & ngram\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.87      0.86     37726\n",
      "           1       0.86      0.84      0.85     37102\n",
      "\n",
      "    accuracy                           0.85     74828\n",
      "   macro avg       0.85      0.85      0.85     74828\n",
      "weighted avg       0.85      0.85      0.85     74828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NB_vect = MultinomialNB()\n",
    "NB_vect.fit(train_X_bigram, train_y)\n",
    "NB_pred_vect_train = NB_vect.predict(train_X_bigram)\n",
    "NB_pred_vect_test = NB_vect.predict(test_X_bigram)\n",
    "print(\"MultinomialNB & ngram\")\n",
    "\n",
    "print(classification_report(test_y, NB_pred_vect_test, target_names=[\"0\",\"1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f82898e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##pipeline to show working well or not\n",
    "a = df[\"tokenized_string\"]\n",
    "b=df[\"comment\"]\n",
    "pipe = Pipeline([('HashingVect',HashingVectorizer()),('LR',LogisticRegression())])\n",
    "pipe.fit(a,b)\n",
    "pipe.predict(['i dont like '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9be974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87e485a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
