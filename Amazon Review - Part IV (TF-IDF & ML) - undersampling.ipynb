{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3080c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.2.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/didemekinci/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/ipykernel_launcher.py:22: FutureWarning:\n",
      "\n",
      "Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "\n",
      "/Users/didemekinci/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning:\n",
      "\n",
      "The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "\n",
      "/Users/didemekinci/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/ipykernel_launcher.py:47: FutureWarning:\n",
      "\n",
      "Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "\n",
    "#plotly library\n",
    "import plotly as py\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from time import time\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "from pprint import pprint\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=1.3)\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.externals \n",
    "import joblib\n",
    "import gensim\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from time import time\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "from pprint import pprint\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=1.3)\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.externals \n",
    "import joblib\n",
    "import gensim\n",
    "\n",
    "#matplotlib library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#word cloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#from a unix time to a date\n",
    "from time import strftime\n",
    "from datetime import datetime\n",
    "import pandas as pd \n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "import string\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns\n",
    "# Importing the Required Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#from xgboost import XGBClassifier\n",
    "#from lightgbm import LGBMModel,LGBMClassifier, plot_importance\n",
    "#from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01316ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1094466 entries, 1 to 1094466\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count    Dtype  \n",
      "---  ------             --------------    -----  \n",
      " 0   marketplace        1094466 non-null  object \n",
      " 1   customer_id        1094466 non-null  int64  \n",
      " 2   review_id          1094466 non-null  object \n",
      " 3   product_id         1094466 non-null  object \n",
      " 4   product_parent     1094466 non-null  int64  \n",
      " 5   product_title      1094466 non-null  object \n",
      " 6   product_category   1094466 non-null  object \n",
      " 7   star_rating        1094464 non-null  object \n",
      " 8   helpful_votes      1094461 non-null  float64\n",
      " 9   total_votes        1094461 non-null  float64\n",
      " 10  vine               1094461 non-null  object \n",
      " 11  verified_purchase  1094461 non-null  object \n",
      " 12  review_headline    1094454 non-null  object \n",
      " 13  review_body        1094449 non-null  object \n",
      " 14  review_date        1094407 non-null  object \n",
      "dtypes: float64(2), int64(2), object(11)\n",
      "memory usage: 133.6+ MB\n"
     ]
    }
   ],
   "source": [
    "mypath=\"/Users/didemekinci/Desktop/Graduate Project/02_data/\"\n",
    "import pandas as pd\n",
    "import glob\n",
    "files = glob.glob(\"/Users/didemekinci/Desktop/Graduate Project/02_data/*.csv\")\n",
    "column_names=['marketplace',\"customer_id\",\"review_id\",\"product_id\",\"product_parent\",\"product_title\",\"product_category\",\"star_rating\",\"helpful_votes\",\"total_votes\",\"vine\",\"verified_purchase\",\"review_headline\",\"review_body\",\"review_date\"]\n",
    "df0=pd.read_csv('/Users/didemekinci/Desktop/Graduate Project/02_data/0000000015.csv',sep=\"\\t\",names=column_names, low_memory=False)\n",
    "df1=pd.read_csv('/Users/didemekinci/Desktop/Graduate Project/02_data/0000000366.csv',sep=\"\\t\",names=column_names, low_memory=False)\n",
    "df2=pd.read_csv('/Users/didemekinci/Desktop/Graduate Project/02_data/0000000115.csv',sep=\"\\t\",names=column_names, low_memory=False)\n",
    "df3=pd.read_csv('/Users/didemekinci/Desktop/Graduate Project/02_data/0000000300.csv',sep=\"\\t\",names=column_names, low_memory=False)\n",
    "df4=pd.read_csv('/Users/didemekinci/Desktop/Graduate Project/02_data/0000000324.csv',sep=\"\\t\",names=column_names, low_memory=False)\n",
    "df5=pd.read_csv('/Users/didemekinci/Desktop/Graduate Project/02_data/0000000247.csv',sep=\"\\t\",names=column_names, low_memory=False)\n",
    "df6=pd.read_csv('/Users/didemekinci/Desktop/Graduate Project/02_data/0000000018.csv',sep=\"\\t\",names=column_names, low_memory=False)\n",
    "df7=pd.read_csv('/Users/didemekinci/Desktop/Graduate Project/02_data/0000000069.csv',sep=\"\\t\",names=column_names, low_memory=False)\n",
    "df8=pd.read_csv('/Users/didemekinci/Desktop/Graduate Project/02_data/0000000125.csv',sep=\"\\t\",names=column_names, low_memory=False)\n",
    "df9=pd.read_csv('/Users/didemekinci/Desktop/Graduate Project/02_data/0000000190.csv',sep=\"\\t\",names=column_names, low_memory=False)\n",
    "df10=pd.read_csv('/Users/didemekinci/Desktop/Graduate Project/02_data/0000000268.csv',sep=\"\\t\",names=column_names, low_memory=False)\n",
    "df = df0.append([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10])\n",
    "df.index = np.arange(1, len(df)+1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "078c2216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORRECTION OF COLUMNS AND TYPES\n",
    "df = df.drop(df[(df.review_date == \"1\") | (df.review_date == \"4\") | (df.review_date == \"2\") | (df.review_date == \"3\") | (df.review_date == \"5\")| (df.review_date.isnull())].index)\n",
    "df = df.drop(df[(df.star_rating == '2015-03-27')|(df.star_rating == \"2015-04-17\") | (df.star_rating == \"2011-07-10\") |  (df.star_rating.isnull())].index)\n",
    "df['helpful_votes'] = df['helpful_votes'].fillna(0)\n",
    "df['total_votes'] = df['total_votes'].fillna(0)\n",
    "df= df.astype({\"customer_id\": int, \"product_parent\": int, \"star_rating\": int, \"helpful_votes\" : int, \"total_votes\":float, \"review_body\": str})\n",
    "df['review_date']=pd.to_datetime(df['review_date'], format='%Y/%m/%d',errors =\"ignore\")\n",
    "\n",
    "#adding review length and binary vectorize star rating by comment columns\n",
    "df[\"review_len\"]=[len(x) for x in df['review_body']]\n",
    "df[\"comment\"] = df['star_rating'].apply(lambda x: 1 if x>3 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4bff18",
   "metadata": {},
   "source": [
    "# DATA CLEANSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "343510d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    187068\n",
       "1    187068\n",
       "Name: comment, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True) #shuffle\n",
    "data=df[df['comment']==0][:187068]\n",
    "data=data.append(df[df['comment']==1][:187068])\n",
    "data = data.reset_index(drop=True)\n",
    "display(data['comment'].value_counts())\n",
    "df=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5cd7aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/didemekinci/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "sw = stopwords.words('english')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from string import punctuation\n",
    "\n",
    "def cleanText(text):\n",
    "    #text = BeautifulSoup(text, \"lxml\",).text\n",
    "    text = text.lower()\n",
    "\n",
    "    text= text.replace('<br />',' ')\n",
    "    text= text.replace('<br/>',' ')\n",
    "\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    # general\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r'\\|\\|\\| ', r'  ', text)\n",
    "    text = re.sub(r'http\\S+', r' <URL> ', text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}^`+=~|.!?,]\", \" \", text) \n",
    "    text = re.sub(r'[^a-zA-z.,!?/:;\\\"\\'\\s]', \" \", text)\n",
    "    text = re.sub(r'[^a-zA-z.,!?/:;\\\"\\'\\s]', \" \", text)\n",
    "    text = re.sub(r'[^\\w]', ' ', text)\n",
    "    text = re.sub(\"\\s\\s+\", \" \", text)\n",
    "   # text=re.sub('(\\\\b[A-Ka-k] \\\\b|\\\\b [A-Ka-k]\\\\b)', '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65628e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_review'] = df['review_body'].apply(cleanText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3e91f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(s):\n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    # remove short words, they're probably not useful\n",
    "    tokens = [t for t in tokens if len(t) > 2]\n",
    "    # put words into base form\n",
    "    #tokens = [t for t in tokens if t not in stopwords.words(\"english\") + [\"\"]]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8191815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_review'] = df['cleaned_review'].apply(tokenize_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "416c2e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokenized_string\"] = df.tokenized_review.apply(lambda x: ' '.join([str(i) for i in x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18c7f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_sentence(s):\n",
    "    \n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t, pos=\"v\") for t in s]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b15b3d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatized_review'] = df['tokenized_review'].apply(lemmatize_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f3f1c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lemmatized_string\"] = df.lemmatized_review.apply(lambda x: ' '.join([str(i) for i in x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8467430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "df['lemmatized_string_wo_sw'] = df['lemmatized_string'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "df[\"tokenized_string_wo_sw\"] = df.tokenized_string.apply(lambda x: ''.join([str(i) for i in x]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaeaa02",
   "metadata": {},
   "source": [
    "# TF IDF with machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23ede23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:((299308,), (299308,))Test: ,((74828,), (74828,))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train, Y_test = train_test_split(df[\"lemmatized_string\"], df[\"comment\"], test_size=0.2, random_state=30)\n",
    "print(f'Train:{X_train.shape,Y_train.shape}Test: ,{X_test.shape,Y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5807844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=3, max_features=2000, stop_words='english')\n",
    "tf_x_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "tf_x_test = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9624397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF IDF SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81     37385\n",
      "           1       0.81      0.81      0.81     37443\n",
      "\n",
      "    accuracy                           0.81     74828\n",
      "   macro avg       0.81      0.81      0.81     74828\n",
      "weighted avg       0.81      0.81      0.81     74828\n",
      "\n",
      "Train ccuracy of SVM : 0.8125275635799912\n",
      "Test accuracy of SVM : 0.8096835409205111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(random_state=0)\n",
    "clf.fit(tf_x_train,Y_train)\n",
    "y_test_pred=clf.predict(tf_x_test)\n",
    "print(\"TF IDF SVC\")\n",
    "print(classification_report(Y_test, y_test_pred, target_names=[\"0\",\"1\"]))\n",
    "SVM_score=clf.score(tf_x_test,y_test_pred)\n",
    "\n",
    "print(\"Train ccuracy of SVM :\",clf.score(tf_x_train,Y_train))\n",
    "print(\"Test accuracy of SVM :\",clf.score(tf_x_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e346e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF IDF LOGREG\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81     37385\n",
      "           1       0.81      0.81      0.81     37443\n",
      "\n",
      "    accuracy                           0.81     74828\n",
      "   macro avg       0.81      0.81      0.81     74828\n",
      "weighted avg       0.81      0.81      0.81     74828\n",
      "\n",
      "Train ccuracy of LOGREG : 0.8128449623798896\n",
      "Test accuracy of LOGREG : 0.8094964451809483\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf2 = LogisticRegression(max_iter=2000,solver=\"lbfgs\")\n",
    "clf2.fit(tf_x_train,Y_train)\n",
    "y_test_pred=clf2.predict(tf_x_test)\n",
    "print(\"TF IDF LOGREG\")\n",
    "print(classification_report(Y_test, y_test_pred, target_names=[\"0\",\"1\"]))\n",
    "Log_score=clf2.score(tf_x_test,y_test_pred)\n",
    "\n",
    "print(\"Train ccuracy of LOGREG :\",clf2.score(tf_x_train,Y_train))\n",
    "print(\"Test accuracy of LOGREG :\",clf2.score(tf_x_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8906065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF IDF MultinomialNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.80     37385\n",
      "           1       0.79      0.80      0.80     37443\n",
      "\n",
      "    accuracy                           0.80     74828\n",
      "   macro avg       0.80      0.80      0.80     74828\n",
      "weighted avg       0.80      0.80      0.80     74828\n",
      "\n",
      "Train ccuracy of MultinomialNB : 0.7970151148649551\n",
      "Test accuracy of MultinomialNB : 0.7970813064628214\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(tf_x_train,Y_train)\n",
    "y_test_pred=MNB.predict(tf_x_test)\n",
    "print(\"TF IDF MultinomialNB\")\n",
    "print(classification_report(Y_test, y_test_pred, target_names=[\"0\",\"1\"]))\n",
    "MNB_Score=MNB.score(tf_x_test,y_test_pred)\n",
    "print(\"Train ccuracy of MultinomialNB :\",MNB.score(tf_x_train,Y_train))\n",
    "print(\"Test accuracy of MultinomialNB :\",MNB.score(tf_x_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9944a242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF IDF DecisionTree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.70      0.70     37385\n",
      "           1       0.70      0.70      0.70     37443\n",
      "\n",
      "    accuracy                           0.70     74828\n",
      "   macro avg       0.70      0.70      0.70     74828\n",
      "weighted avg       0.70      0.70      0.70     74828\n",
      "\n",
      "Train ccuracy of decision tree: 0.9899067181632298\n",
      "Test accuracy of decision tree: 0.701114556048538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import  DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(tf_x_train,Y_train)\n",
    "y_test_pred=dt.predict(tf_x_test)\n",
    "print(\"TF IDF DecisionTree\")\n",
    "print(classification_report(Y_test, y_test_pred, target_names=[\"0\",\"1\"]))\n",
    "#dt.fit(x_train,y_train)\n",
    "#y_pred=dt.predict(x_test)\n",
    "DecisionTree_score=dt.score(tf_x_test,y_test_pred)\n",
    "#DecisionTree_score=dt.score(x_test,y_test)\n",
    "print(\"Train ccuracy of decision tree:\",dt.score(tf_x_train,Y_train))\n",
    "print(\"Test accuracy of decision tree:\",dt.score(tf_x_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b619a247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF IDF GaussianNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.80     37385\n",
      "           1       0.79      0.80      0.80     37443\n",
      "\n",
      "    accuracy                           0.80     74828\n",
      "   macro avg       0.80      0.80      0.80     74828\n",
      "weighted avg       0.80      0.80      0.80     74828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb= GaussianNB()\n",
    "nb.fit(tf_x_train.todense(),Y_train)\n",
    "y_test_pred=MNB.predict(tf_x_test)\n",
    "print(\"TF IDF GaussianNB\")\n",
    "print(classification_report(Y_test, y_test_pred, target_names=[\"0\",\"1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e527bced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF IDF RandomForest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.81      0.79     37385\n",
      "           1       0.80      0.76      0.78     37443\n",
      "\n",
      "    accuracy                           0.79     74828\n",
      "   macro avg       0.79      0.79      0.79     74828\n",
      "weighted avg       0.79      0.79      0.79     74828\n",
      "\n",
      "Train ccuracy of RF: 0.9892050997634544\n",
      "Test accuracy of RF: 0.7851739990377934\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf=RandomForestClassifier(n_estimators=30,random_state = 3)\n",
    "rf.fit(tf_x_train,Y_train)\n",
    "y_test_pred=rf.predict(tf_x_test)\n",
    "print(\"TF IDF RandomForest\")\n",
    "print(classification_report(Y_test, y_test_pred, target_names=[\"0\",\"1\"]))\n",
    "rf_score=rf.score(tf_x_test,y_test_pred)\n",
    "print(\"Train ccuracy of RF:\",rf.score(tf_x_train,Y_train))\n",
    "print(\"Test accuracy of RF:\",rf.score(tf_x_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92bd0868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF IDF KNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.88      0.69     37385\n",
      "           1       0.73      0.33      0.46     37443\n",
      "\n",
      "    accuracy                           0.61     74828\n",
      "   macro avg       0.65      0.61      0.57     74828\n",
      "weighted avg       0.65      0.61      0.57     74828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "\n",
    "knn.fit(tf_x_train,Y_train)\n",
    "y_test_pred = knn.predict(tf_x_test)\n",
    "print(\"TF IDF KNN\")\n",
    "print(classification_report(Y_test, y_test_pred, target_names=[\"0\",\"1\"]))\n",
    "#knn_score=knn.score(tf_x_test,y_test_pred)\n",
    "#print(\"Train ccuracy of KNN-3 :\",knn.score(tf_x_train,Y_train))\n",
    "#print(\"Test accuracy of KNN-3:\",knn.score(tf_x_test,Y_test))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e3bd8e1",
   "metadata": {},
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "knn.fit(tf_x_train,Y_train)\n",
    "y_test_pred = knn.predict(tf_x_test)\n",
    "print(\"TF IDF KNN, n=5\")\n",
    "print(classification_report(Y_test, y_test_pred, target_names=[\"0\",\"1\"]))\n",
    "#knn_score=knn.score(tf_x_test,y_test_pred)\n",
    "#print(\"Train ccuracy of KNN-5 :\",knn.score(tf_x_train,Y_train))\n",
    "#print(\"Test accuracy of KNN-5:\",knn.score(tf_x_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9dbb4822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF IDF SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81     37385\n",
      "           1       0.81      0.81      0.81     37443\n",
      "\n",
      "    accuracy                           0.81     74828\n",
      "   macro avg       0.81      0.81      0.81     74828\n",
      "weighted avg       0.81      0.81      0.81     74828\n",
      "\n",
      "Train ccuracy of RF: 0.8125275635799912\n",
      "Test accuracy of RF: 0.8096835409205111\n"
     ]
    }
   ],
   "source": [
    "SVM_vect = LinearSVC(random_state=0)\n",
    "SVM_vect.fit(tf_x_train,Y_train)\n",
    "y_test_pred = SVM_vect.predict(tf_x_test)\n",
    "print(\"TF IDF SVC\")\n",
    "print(classification_report(Y_test, y_test_pred, target_names=[\"0\",\"1\"]))\n",
    "\n",
    "svc_score=SVM_vect.score(tf_x_test,y_test_pred)\n",
    "print(\"Train ccuracy of RF:\",SVM_vect.score(tf_x_train,Y_train))\n",
    "print(\"Test accuracy of RF:\",SVM_vect.score(tf_x_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d696c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00aaccd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##pipeline to show working well or not\n",
    "a = df['lemmatized_string']\n",
    "b=df[\"comment\"]\n",
    "pipe = Pipeline([('tfid',TfidfVectorizer()),('LR',LogisticRegression())])\n",
    "pipe.fit(a,b)\n",
    "pipe.predict(['like '])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5f5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
